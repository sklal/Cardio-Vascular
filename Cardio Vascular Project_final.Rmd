---
title: "DTAS 6101"
#author: "Mishkin Khunger, Sanat Lal, Sharmin Kantharia, Vishal Pathak"
output: html_document
---

```{r setup, include=FALSE ,echo=FALSE}
#```{r setup, include=FALSE ,echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# {.tabset .tabset-fade .tabset-pills}
***

## **Chapter 1: Introduction** 

Cardiovascular disease (CVD) is a class of disease that involves the heart or blood vessels. CVD includes coronary artery diseases (CAD) such as Angina and Myocardial Infarction (commonly known as heart attack).

Other CVDs include stroke, heart failure, hypertensive heart disease, rheumatic heart disease, cardiomyopathy, abnormal heart rhythms, congenital heart disease, etc.

On a global scale, CVDs are the leading cause of death. Altogether, CVD has resulted in 17.9 million deaths (32.1%) in 2015, which is an increment from 12.3 million (25.8%) in 1990.

Deaths at a given age, from CVD, are more common and have been increasing in today’s day and age.

Through this project and analysis, we aim to identify the factors which cause Cardiovascular diseases in adults.



### SMART QUESTIONS

1. Can age be a factor?
2. How important are alcohol and smoking in affecting Cardiovascular Diseases?
3. Apart from the most obvious factors, what other aspects of Gender can affect the risk of CVD?




```{r dataload, include=FALSE, echo=TRUE}
library(ggplot2)


cd = read.csv("D:/Data Science/Cardio Vascular/Cardio-Vascular/cardio.csv")

dim(cd)

cd$age<- (cd$age/365)

cd$age<- round(cd$age) 
```

```{r basicchecks, include=FALSE, echo=TRUE}

#Structure
str(cd)

#Summary
summary(cd)

# Anomalies found in height, weight, systolic (ap_hi) and diastolic (ap_lo)
#Gender,cholestrol,gluc,smoke,alco,active,cardio are needed to be converted into categorical variable

```
```{r lib, include=FALSE, echo=TRUE}
library(pastecs)
library(psych)
library(Hmisc)
```

## **Chapter 2: Data Cleaning** 

```{r Outlierdetectionofage, include=TRUE, echo=TRUE}
fivenum(cd$age)
psych::describe(cd$age)
stat.desc(cd$age)
hist(cd$age,
     main = "Histogram of Age",
     xlab = "Age in Years")
boxplot(cd$age,
        main = toupper("Boxplot of Age"),
        ylab = "Age in years",
        col = "blue")
outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
As we can see skewness in -0.3 which means age is approximately symmetric. As if skewness is in between scale of (-0.5 to 0.5)it will be considered approximately symmetric.


#### so in age as we can see the outliers are value 30 which are outside of 1.5*IQR . We are going to use capping methiod in order to treat outlier. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile.

```{r Outliertreatmentofage, include=TRUE, echo=TRUE}
qnt <- quantile(cd$age, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$age, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$age, na.rm = T)
cd$age[cd$age < (qnt[1] - H)] <- caps[1]
cd$age[cd$age > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$age)$out  # outlier values.
boxplot(cd$age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

``` 
#### As we can see now the age column has no outliers.we are good to go.

```{r HeightEDA, include=TRUE, echo=TRUE}
fivenum(cd$height)
psych::describe(cd$height)
stat.desc(cd$height)

hist(cd$height,
     main = "Histogram of Height",
     xlab = "Height in centimeter")

outlier_values <- boxplot.stats(cd$cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$height))

```
#### As we can see there are few of outliers in Height variable , which is around .004 percent of total data. since this is continous data so we are going treat with capping method. For missing values that lie outside the 1.5*IQR limits, we could cap it by replacing those observations outside the lower limit with the value of 5th %ile and those that lie above the upper limit, with the value of 95th %ile. Moreover the skewness is around -0.63 which means height column is left skewed.

```{r Heightoutlierimpute, include=TRUE, echo=TRUE}

qnt <- quantile(cd$height, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$height, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$height, na.rm = T)
cd$height[cd$height < (qnt[1] - H)] <- caps[1]
cd$height[cd$height > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

```
#### As we can see now there are no outliers in height column.Also  earlier there was skewness in the column and it was (-0.63) but after the outlier treatment it reduces to 0.09 . so height variable is not skewed.so we are good to go.


```{r WeightEDA, include=TRUE, echo=TRUE}
fivenum(cd$weight)
psych::describe(cd$weight)
stat.desc(cd$weight)

hist(cd$weight,
     main = "Histogram of Weight",
     xlab = "Weight in kg")

outlier_values <- boxplot.stats(cd$weight)$out  # outlier values.
boxplot(cd$weight, main="Weight", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$weight))

```
#### As we can see there are 10 % of data which are outlier . Also data is highly asymmetric as skew value is (1.01) which meamns age is highly right skewed.we will first treat outlier ,then we will check if skewness persists then we will apply transformation on the weight column.



```{r Weightoutlierimpute, include=TRUE, echo=TRUE}

qnt <- quantile(cd$weight, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$weight, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$weight, na.rm = T)
cd$weight[cd$weight < (qnt[1] - H)] <- caps[1]
cd$weight[cd$weight > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$height)$out  # outlier values.
boxplot(cd$height, main="Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$weight)

```
#### As we can see now there are no outliers and also skewness is 0.39 which means our data is approximately normal.we are good to go.

```{r ap_hi(EDA), include=TRUE, echo=TRUE}
fivenum(cd$ap_hi)
psych::describe(cd$ap_hi)
stat.desc(cd$ap_hi)

hist(cd$ap_hi,
     main = "Histogram of systolic Blood Pressure",
     xlab = "Ap_hi")

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_hi))

```
#### As we can see there is a 8 units of difference in mean and Meadian.which is really high . But the outliers are only around .05 % . This means that the values of outliers are of very values. Aslo the skew and kurtosis (85.29,7579.32)values are really very high. which again point to the fact that some values are incorrect or garbage values . we are to going treat the outliers in the next section and then we will analyse our column again.

```{r SBPoutlierimpute, include=TRUE, echo=TRUE}

qnt <- quantile(cd$ap_hi, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_hi, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_hi, na.rm = T)
cd$ap_hi[cd$ap_hi < (qnt[1] - H)] <- caps[1]
cd$ap_hi[cd$ap_hi > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_hi)$out  # outlier values.
boxplot(cd$ap_hi, main="systolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_hi)

```

#### yes we were absolutely right the outliers were of very high and they were garbage value.After treating outliers , we can see our sweet boxplot(without any outliers) .Moreover our skew value is now under approximately normal range which is 0.54.

```{r ap_low(EDA), include=TRUE, echo=TRUE}
fivenum(cd$ap_lo)
psych::describe(cd$ap_lo)
stat.desc(cd$ap_lo)

hist(cd$ap_lo,
     main = "Histogram of Diastolic Blood Pressure",
     xlab = "Ap_low")

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

outliersper <- function(x){
  length(which(x >  mean(x) + 3 * sd(x) | x < mean(x) - 3 * sd(x))  ) / length(x)
}

print(outliersper(cd$ap_lo))

```

#### As we can see there is difference of 16 units in mean and median.From the histogram we can see that data is right skewed which is around (32.11). Also the kurtosisis really high. there are around 10% outliers so we are going to treat it.after that we are going to check again

```{r DBDoutliertreatment, include=TRUE, echo=TRUE}

qnt <- quantile(cd$ap_lo, probs=c(.25, .75), na.rm = T)
caps <- quantile(cd$ap_lo, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(cd$ap_lo, na.rm = T)
cd$ap_lo[cd$ap_lo < (qnt[1] - H)] <- caps[1]
cd$ap_lo[cd$ap_lo > (qnt[2] + H)] <- caps[2]

outlier_values <- boxplot.stats(cd$ap_lo)$out  # outlier values.
boxplot(cd$ap_lo, main="Diastolic Blood Pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)

psych::describe(cd$ap_lo)

```
#### As we can see now after treatment of outliers , skew has also been in expected range(0.48) . now our variable is approximately normally distributed.

#### Missing Value check and treatment on continous variables
```{r Missingvalues, include=TRUE, echo=TRUE}

sum(is.na(cd$age))
sum(is.na(cd$height))
sum(is.na(cd$weight))
sum(is.na(cd$ap_hi))
sum(is.na(cd$ap_lo))

```

```{r genderEDA, include=FALSE, echo=TRUE}
cd[cd$gender==1,]$gender<-"F"
cd[cd$gender==2,]$gender<-"M"
cd$gender<-as.factor(cd$gender)
table(factor(cd$gender))
```

#### There are no missing variables in the data , which means that we are good to go for our bivariate analysis.



```{r Checkaphivsaplo, include=FALSE, echo=TRUE}

temp_min = pmin(cd$ap_hi, cd$ap_lo)
cd$ap_hi = pmax(cd$ap_hi, cd$ap_lo)
cd$ap_lo = temp_min
```
#### since diastolic and systolic are connected and diastolic will always be greater than systolic . so for sanity we have implement few changes to swap values if sysytolic is less than diastolic



```{r fac, include=FALSE, include=FALSE, echo=TRUE}
#Gender
cd$gender<-as.factor(cd$gender)
#Cholestrol
cd$cholesterol<-as.factor(cd$cholesterol)
#Glucose
cd$gluc<-as.factor(cd$gluc)
#Smoke
cd$smoke<- as.factor(cd$smoke)
#Alcohol
cd$alco<- as.factor(cd$alco)
#Active
cd$active<- as.factor(cd$active)
#Cardio
cd$cardio<- as.numeric(cd$cardio)
```



```{r cardioEDA, include=FALSE, echo=TRUE}
cd[cd$cardio==0,]$cardio<-"Negative"
cd[cd$cardio==1,]$cardio<-"Positive"
cd$cardio<-as.factor(cd$cardio)
table(factor(cd$cardio))
```



#### There are no missing variables in the data , which means that we are good to go for our bivariate analysis.



#### Calculating BMI

```{r bmi, echo=FALSE}
cd$bmi<- cd$weight/(cd$height*cd$height)*10000 # Weight in kgs and height in cms

```
#### Dropping the id column
```{r drop id, echo=FALSE}
cd$id<- NULL


```

```{r levels, include=FALSE, echo=TRUE}
levels(cd$gender)
levels(cd$cholesterol)
levels(cd$gluc)
levels(cd$smoke)
levels(cd$alco)
levels(cd$active)

```

```{r dummy variables, include=FALSE, echo=TRUE}
cd$cholnormal  <- ifelse(cd$cholesterol == 1, 1, 0)
cd$cholabovenorm <- ifelse(cd$cholesterol == 2, 1, 0)
sum(cd$cholabovenorm)
sum(cd$cholnormal)

cd$glucnorm <- ifelse(cd$gluc == 1, 1, 0)
cd$glucabovenorm <- ifelse(cd$gluc == 2, 1, 0)
sum(cd$glucnorm)
sum(cd$glucabovenorm)

```

#### so we have create 2 dummy variables for cholestrol and 2 dummy variaables for glucose category and in both the columns there are 3 levels


## **Chapter 3: Graphical Representation** 

```{r visualization1,  echo=FALSE}
library(ggplot2)

#age count , affected vs not affected
counts <- table(cd$cardio, cd$age)
barplot(counts, main="Age distribution",
  xlab="Age", ylab="Counts", col=c("darkblue","red"),
  legend = rownames(counts), beside=TRUE)


```


#### We see that most people who are suffering from cardio vascular diseases are of the age 60, followed by 56.Majorly, people belonging to the age group 50+ are suffering from the disease.



```{r v2,  echo=FALSE}


#plot(cd$age,cd$ap_hi, main="Scatterplot Example",
 #  xlab="Car Weight ", ylab="Miles Per Gallon ")

library(car)
scatterplot(cd$ap_hi ~ cd$age, data=cd,
   xlab="Age", ylab="Systolic Blood pressure",
   main="Relationship of age and systolic blood pressure",
   )


mytable <- table(cd$cholesterol )
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
   main="Pie Chart of Cholestrol levels")

```


#### There are three cholestrol levels-
#### 1- Normal
#### 2- Above Normal
#### 3- Way above normal

#### The above pie chart shows that 3/4th of the population come under the normal cholestrol category.We see that most people who are suffering from cardio vascular diseases are of the age 60, followed by 56.Majorly, people belonging to the age group 50+ are suffering from the disease.


```{r v3, echo=FALSE}
library("ggplot2")

    ggplot(cd, aes(x= cholesterol,  group=cd$gender)) + 
    geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= "count", vjust = -.5) +
    labs(y = "Percent", fill="cholestrol") +
    facet_grid(~gender) +
    scale_y_continuous(labels = scales::percent)

```

#### The above graph shows that there are comparitively higher chances of women to have a cardio vascular disease where 76.9% of the women have normal cholestrol level.


```{r v4, echo=FALSE}

library("ggplot2")
attach(cd)
plot(gender, cardio, main="Scatterplot Example",
   xlab="gender ", ylab="Cardio ", pch=19)

counts <- table(cd$cardio, cd$gender)
barplot(counts, main="Gender distribution",
  xlab="Gender", ylab="Counts", col=c("pink","red"),
  legend = rownames(counts), beside=TRUE)

```

#### The above graph suggests that there are higher number of cases for women, but there are higher number of negative cases for women and higher number of positive cases for men.


```{r glvscd, echo=FALSE}
  
  library("ggplot2")
  attach(cd)
  
  counts <- table(cd$cardio, cd$gluc)
  barplot(counts, main="Glucose distribution",
    xlab="Glucose", ylab="Counts", col=c("blue","black"),
    legend = rownames(counts), beside=TRUE)

```


#### There are three Glucose levels-
#### 1- Normal
#### 2- Above Normal
#### 3- Way above normal

#### The above graph suggests that, the majority of the population comes under normal glucose level. Additionally, for normal glucose levels the chances of having cardio-vascular disease is less whereas for above normal glucose levels the chances of having a cardio-vascular disease is more.

```{r chcd, echo=FALSE}
  
  library("ggplot2")
  attach(cd)

  
  counts <- table(cd$cardio, cd$cholesterol)
  barplot(counts, main="Cholestrol vs Cardio",
    xlab="Cholesterol", ylab="Counts", col=c("black","red"),
    legend = rownames(counts), beside=TRUE)
```


#### The above graph suggests that, the majority of the population comes under normal glucose level. Additionally, for normal Cholesterol levels the chances of having cardio-vascular disease is less whereas for above normal cholesterol levels the chances of having a cardio-vascular disease is more.  

## **Chapter 4: Data Division & Statistical Tests** 

  
```{r dividedata,echo= FALSE}
require(caTools) #install caTools
set.seed(123)
sample = sample.split(cd,SplitRatio = 0.70)
train_cd =subset(cd,sample ==TRUE)
test_cd =subset(cd, sample==FALSE)
```

#### we are dividing the data into test and train.


```{r stat, echo= FALSE}

# Relationship with the target variable


ttestresage = t.test(train_cd$age~train_cd$cardio)
ttestresage
```


#### Strong significance

#### Statistical Significance between Height & Cardio Vascular Disease risk 
```{r hvscd, echo= FALSE}
ttestresheight = t.test(train_cd$height~train_cd$cardio)
ttestresheight
```



#### Comparitively weak significance

#### Statistical Significance between Weight & Cardio Vascular Disease risk 
```{r wvscd, echo= FALSE}
###```{r wvscd, include=FALSE, echo=TRUE}
ttestresweight = t.test(train_cd$weight~train_cd$cardio)
ttestresweight
```


#### Strong significance

```{r bvscd, echo= FALSE}
##```{r bvscd, include=FALSE, echo=TRUE}
#Statistical Significance between BMI & Cardio Vascular Disease risk 
ttestresbmi = t.test(train_cd$bmi~train_cd$cardio)
ttestresbmi
```


#### Strong significance

#### Statistical Significance between Systolic & Cardio Vascular Disease 
```{r svdcd, echo=FALSE}
#```{r svdcd, include=FALSE, echo=TRUE}
ttestressys = t.test(train_cd$ap_hi~train_cd$cardio)
ttestressys 
```


#### Strong significance

#### Statistical Significance between Diastolic & Cardio Vascular Disease 
```{r dvscd, echo=FALSE}
#```{r dvscd, include=FALSE, echo=TRUE}
ttestresdia = t.test(train_cd$ap_lo~train_cd$cardio)
ttestresdia 
```

#### Strong significance



#### Statistical Significance between Cholestrol & Cardio Vascular Disease
```{r cvdcd, echo=FALSE}
#```{r cvdcd, include=FALSE, echo=TRUE}
chisqrescholesterol = chisq.test(train_cd$cholesterol,train_cd$cardio)
chisqrescholesterol 
```


#### Strong significance

#### Statistical Significance between Glucose & Cardio Vascular Disease
```{r gvscd, echo=FALSE}
#```{r gvscd, include=FALSE, echo=TRUE}

chisqresgluc = chisq.test(train_cd$gluc,train_cd$cardio)
chisqresgluc 
```

#### Strong significance

#### Statistical Significance between Glucose & Cardio Vascular Disease
```{r avscd, echo=FALSE}
#```{r avscd, include=FALSE, echo=TRUE}
chisqresalco = chisq.test(train_cd$alco,train_cd$cardio)
chisqresalco 
```

#### Not statistically significant but missing by a small margin

#### Statistical Significance between Physical Activity & Cardio Vascular Disease risk 
```{r scvscd, echo=FALSE}
#```{r scvscd, include=FALSE, echo=TRUE}
chisqresact = chisq.test(train_cd$active,train_cd$cardio)
chisqresact
```


#### Strong significance

## **Chapter 5: Models** 
 

#### Logistic Regression

```{r logisticmodel, echo=FALSE }
#```{r avscd, echo= FALSE}
#```{r logisticmodel, include=FALSE, echo=TRUE}

fit <- glm(cardio ~ bmi+weight+ap_hi+height+glucnorm+glucabovenorm+smoke+alco+active+cholnormal+cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit)

```


#### As you can see the factors we have used to build model is giving results . For most of the variables the p value is low that means they are effective in predicting the cario deseas e. bmi and height are 2 variables which are not significant . alos the diffrence in value of null deviance and residual deviance is high . which means model is fitting.

```{r letscheck,  echo=FALSE}
#```{r letscheck, include=FALSE, echo=TRUE}

require(MASS)
step3<- stepAIC(fit,direction="both")
?stepAIC()
ls(step3)
step3$anova

```   


#### As we can see the step aic functio has removed bmi , from the model and gave our best predictors.Now we gonna run our model again by removing BMI and compare it as well.


```{r secondmodel,  echo=FALSE}
#```{r secondmodel, include=FALSE, echo=TRUE}
fit2<-glm(cardio ~ weight + ap_hi + height + glucnorm + glucabovenorm + 
    smoke + alco + active + cholnormal + cholabovenorm , data = train_cd , family = binomial(logit) )
summary(fit2)
```


#### As we can see there AIC value for 1st and second model is almost same which means BMI was not impacting the model.

```{r letscheckhl, echo=FALSE}
library(ResourceSelection)
hl <- hoslem.test(train_cd$cardio, fitted(fit2), g = 10)

cbind(hl$expected , hl$observed)

```


```{r letscheckconcordance, echo=FALSE}
Concordance = function(GLM.binomial) {
  outcome_and_fitted_col = cbind(GLM.binomial$y, GLM.binomial$fitted.values)
  # get a subset of outcomes where the event actually happened
  ones = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 1,]
  # get a subset of outcomes where the event didn't actually happen
  zeros = outcome_and_fitted_col[outcome_and_fitted_col[,1] == 0,]
  # Equate the length of the event and non-event tables
  if (length(ones[,1])>length(zeros[,1])) {ones = ones[1:length(zeros[,1]),]}
  else {zeros = zeros[1:length(ones[,1]),]}
  # Following will be c(ones_outcome, ones_fitted, zeros_outcome, zeros_fitted)
  ones_and_zeros = data.frame(ones, zeros)
  # initiate columns to store concordant, discordant, and tie pair evaluations
  conc <- rep(NA, length(ones_and_zeros[,1]))
  disc <- rep(NA, length(ones_and_zeros[,1]))
  ties = rep(NA, length(ones_and_zeros[,1]))
  for (i in 1:length(ones_and_zeros[,1])) {
    # This tests for concordance
    if (ones_and_zeros[i,2] > ones_and_zeros[i,4])
    {conc[i] = 1
    disc[i] = 0
    ties[i] = 0}
    # This tests for a tie
    else if (ones_and_zeros[i,2] == ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 0
      ties[i] = 1
    }
    # This should catch discordant pairs.
    else if (ones_and_zeros[i,2] < ones_and_zeros[i,4])
    {
      conc[i] = 0
      disc[i] = 1
      ties[i] = 0
    }
  }
  # Here we save the various rates
  conc_rate = mean(conc, na.rm=TRUE)
  disc_rate = mean(disc, na.rm=TRUE)
  tie_rate = mean(ties, na.rm=TRUE)
  Somers_D<-conc_rate - disc_rate
  gamma<- (conc_rate - disc_rate)/(conc_rate + disc_rate)
  #k_tau_a<-2*(sum(conc)-sum(disc))/(N*(N-1)
  return(list(concordance=conc_rate, num_concordant=sum(conc), discordance=disc_rate, num_discordant=sum(disc), tie_rate=tie_rate,num_tied=sum(ties),
              somers_D=Somers_D, Gamma=gamma))
  
}
Concordance(fit2) 
require(car)
vif(fit2)

  
```

#### adj-R square would be totally irrelevant in case of logistic regression because we model the log odds ratio and it becomes very difficult in terms of explain ability.This is where concordance steps in to help. Concordance tells us the association between actual values and the values fitted by the model in percentage terms. Concordance is defined as the ratio of number of pairs where the 1 had a higher model score than the model score of zero to the total number of 1-0 pairs possible. A higher value for concordance (60-70%) means a better fitted model. However, a very large value for concordance (85-95%) could also suggest that the model is over-fitted and needs to be re-aligned to explain the entire population. so as we can see our model have 77 % of concordance whcih means its a good fit.

#### Moving over somers d if you see it is around 0.55 . Somers’ D is an index that you want to be closer to 1 and farther from −1. So we can say that our model is good fitted .

#### Now we are going to see the AUC And KS statsics for which we will need ROCR Package.

```{r Predict, echo=FALSE}

pred <- predict(fit2 , newdata = test_cd,type = 'response')
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- test_cd$cardio


#mean(y_pred == y_act)

```
#### As we can see in prediction we have an accuracy rate of 72 %

```{r completely, echo=FALSE}
library(sqldf)
t1<-cbind(train_cd, Prob=predict(fit2,train_cd , type="response" ))
names(t1)

mean(t1$Prob)
View(t1)

t2<-cbind(test_cd,  Prob=predict(fit2,test_cd, type="response"))  

mean(t2$Prob)
View(t2)

#Decile Analysis Reports - t1(training)

# find the decile locations 
decLocations <- quantile(t1$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t1$decile <- findInterval(t1$Prob,c(-Inf,decLocations, Inf))

t1_DA <-sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
              , sum(cardio) as default_cnt
              from t1
              group by decile
              order by decile desc")

View(t1_DA) 
write.csv(t1_DA,"mydata1_DA.csv")


#Decile Analysis Reports - t2(testing)

# find the decile locations 
decLocations <- quantile(t2$Prob, probs = seq(0.1,0.9,by=0.1))

# use findInterval with -Inf and Inf as upper and lower bounds
t2$decile <- findInterval(t2$Prob,c(-Inf,decLocations, Inf))


t2_DA <- sqldf("select decile, count(decile) as count, min(Prob) as Min_prob
                     , max(Prob) as max_prob 
                    , sum(cardio) as default_cnt
                    from t2
                    group by decile
                    order by decile desc")
View(t2_DA)
```




```{r cdch, echo=FALSE }

cd$cardio<-as.factor(cd$cardio)

```

#### Decision Tree

### We use the packages 'rpart' and 'rpart.plot' inorder to compute and plot the decision trees. The rpart() uses our target feature (cardio) against other features. 

#### In the first tree, we compare the target against all the other features in the dataset. Using printcp() and plotcp() we can see the most important features we want to work with. As we can see, our most important deciding feature is ap_hi (systolic blood pressure). 

#### Using cross-validation, we can see the most important variables using complexity parameters. The optimized tree shows us that only one deciding feature is the basis to deicide whether a person has a CVD or not. However, we can also see that the oroginal tree has 5 most important features: ap_hi, ap_lo, cholestrol, age and bmi.

#### We use the packages 'rattle', 'rpart.plot' and 'RColorBrewer', to create a more clean tree with the different probablilities of the possiblilities of Yes and No of attaining CVD.

#### To check for accuracy, use the predict() and use the accuracy formula.

#### The accuracy of this decision tree is 71.66%


```{r DT,  echo=FALSE}
#install.packages("party")
#install.packages("rpart")
library("rpart")
library("rpart.plot")

# Creating the tree
dt <- rpart(cardio ~ ., method = "class", data = train_cd) 
rpart.plot(dt, extra = 106)

# Plotting the tree
plot(dt)
text(dt, pretty = 0)

# Required libraries
#install.packages("rattle")
library("rattle")
library("rpart.plot")
library("RColorBrewer")

fancyRpartPlot(dt)

# Cross Validation 
printcp(dt)
plotcp(dt)
summary(dt)

# Accuracy

predict_dt <-predict(dt, test_cd, type = 'class')

table_mat <- table(test_cd$cardio, predict_dt)
table_mat

accuracy_dt <- sum(diag(table_mat)) / sum(table_mat)
accuracy_dt

```

#### Random Forest

```{r random forest,  echo=FALSE}
library(randomForest)
set.seed(123)
rf_cd <- randomForest(cardio~.-weight-height, data=train_cd,
                   ntree = 500,
                   mtry = 3,
                   importance = TRUE)
print(rf_cd)
attributes(rf_cd)

```

#### Taking Age,Gender, Blood Pressure (Systolic and Diastolic), Alcohol,Smoking,Glucose,Active and BMI as predictors we get an Out of Bag ( set of boostrap datasets which does not contain  certain records from the original dataset) error rate of 26.68%


```{r oober, echo=FALSE}

oob.error.data <- data.frame(
  Trees=rep(1:nrow(rf_cd$err.rate), times=3),
  Type=rep(c("OOB", "Negative", "Positive"), each=nrow(rf_cd$err.rate)),
  Error=c(rf_cd$err.rate[,"OOB"], 
    rf_cd$err.rate[,"Negative"], 
    rf_cd$err.rate[,"Positive"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))
```

#### Here we can see that error rate is the most stable around 500 trees for both the results as well as OOB. Therefore, we will continue with the  default value.

```{r oob,  echo=FALSE}
oob.rf_cd <- vector(length=10)
for(i in 1:10) {
  temp.model <- randomForest(cardio ~ .-weight-height, data=train_cd, mtry=i, ntree=500)
  oob.rf_cd[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]
}
summary(oob.rf_cd)
```

#### Here we can see that, we get the best result for mtry= 3 in terms of Out of Bag error rate

```{r pred1,  echo=FALSE}
library(caret)
p1 <- predict(rf_cd, train_cd)
t11 <-confusionMatrix(p1, train_cd$cardio)
```

#### The training set gives an accuracy of 76.86% 

```{r pred2,  echo=FALSE}
library(caret)
p2 <- predict(rf_cd, test_cd)
t2<- confusionMatrix(p2, test_cd$cardio)

```

#### The test set gives an accuracy of 73.58%
```{r varimpo, echo=FALSE}
hist(treesize(rf_cd),
     main = "No. of Nodes for the Trees",
     col = "green")

# Variable Importance
varImpPlot(rf_cd,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
importance(rf_cd)
varUsed(rf_cd)

```


#### The variable importance plot shows that in terms of Mean Decrease Gini value we get Systolic(Ap_hi), Diastolic(Ap_lo),bm, age and cholesterol to be our top 5 predictors


## **Chapter 5: Top performing features** 

#### Comparing our 3 models, we found that the most important features are:
#### Ap_hi (Systolic blood pressure)
#### Cholesterol
#### Age 
#### Ap_lo (Diastolic blood pressure)
#### BMI
#### Glucose

## **Chapter 6: Limitations** 

#### Lack of more detailed factors such as genetics (family history), complains of chest pain, shock factor, etc.
#### Time periods for when a factor acted up were not available.
#### Separate data for both types of cholesterol (HDL and LDL), would have helped better in the classification.


## **Chapter 7: Conclusion** 

#### Through our analysis, it was clear that most of the important features that run a risk to affect CVD are those that can be directly or indirectly affected by our health choices.
#### Some factors, however, are not in our control. After the age of 55, our chances to acquire a CVD increases greatly. This risk is greater for men than women between the ages of 40 – 59.
#### BMI is another factor that affects getting CVD. Considering this factor in terms of gender, middle-aged women tend to have a higher BMI than men. This is because, the body fat mass in middle-aged women tends to increase in general. M (30%) W (42%).
#### Features such as alcohol, smoking and physical activity are indirectly related to running a risk of CVD. 

#### In general take care of your health and be happy.
